## 3장 : 컨테이너를 다루는 표준 아키텍처, 쿠버네티스

- 컨테이너 인프라 환경이란?
```json
리눅스 운영 체제의 커널 하나에서 여러 개의 컨테니어가 격리된 상태로 실행되는 인프라 환경
```
개인 환경에서는 1명의 관리자가 다양한 응용프로그램을 사용하므로 각각의 프로그램을 컨테이너로 구현할 필요는 없습니다

하지만, 기업 환경에서는 다수의 관리자가 많은 양의 서버를 함께 관리하기 때문에 **일관성**을 유지하는 것이 매우 중요합니다

기존에는 컨테이너 인프라 환경이 주는 장점이 많았지만, 컨테이너 관리 문제가 까다로워서 보편화되기 어려웠습니다

여기서, **쿠버네티스**가 등장하면서 컨테이너 인프라 환경을 좀 더 효율적으로 관리할 수 있게 됐습니다

이를 계기로 여러 기능들이 계속 추가되면서 쿠버네티스의 생태계가 풍부해졌고, 사실상 쿠버네티스는 컨테이너 인프라 관리 솔루션의 표준이 되었습니다

## 3.3 : 쿠버네티스 연결을 담당하는 서비스

- 쿠버네티스에서의 서비스

외부에서 쿠버네티스 클러스터에 접속하는 방법

```
서비스를 '소비를 위한 도움을 제공한다'는 관점으로 바라본다면 

쿠버네티스가 외부에서 쿠버네티스 클러스터에 접속하기 위한 '서비스'를 제공한다고 볼 수 있습니다
```

### 3.3.1 : 가장 간단하게 연결하는 노드포트

- 노드포트 서비스

외부에서 쿠버네티스 클러스터의 내부에 접속하는 가장 쉬운 방법
노드포트를 설정하면 모든 워커 노드의 특정 포트를 열고 여기로 오는 모든 요청을 노드포트 서비스로 전달합니다

- 부하 분산

파드를 추가를 통해서 부하 분산이 가능하다
추가된 파드를 외부에서 추적해서 접속할 수 있는 방법 중 1개는
오브젝트 스펙에 적힌 이름과, 디플로이먼트 이름을 확인해서 동일하면 같은 파드라고 간주해서 추적할 수 있다

- expose로 노드포트 서비스 생성하기

노드포트 서비스는 오브젝트 스펙 뿐만 아니라 expose 명령어를 통해 생성할 수 있다

```shell

expose 명령어를 사용해 서비스로 내보낼 디플로이먼트

kubectl expose deployment np-pods --type=NodePort --name=np-svc-v2 --port=80

```

### 3.3.2 : 사용 목적별로 연결하는 인그레스

노드포트 서비스는 포트를 중복 사용할 수 없어서 1개의 노드포트에 1개의 디플로이먼트만 적용할 수 있습니다

만약, 여러 개의 디플로이먼트가 있을 때 그 수만큼 노드포트 서비스를 구동해야 한다면 비효율적일 수 있습니다

이를 위해 쿠버네티스는 인그레스를 사용합니다

**인그레스**는 고유한 주소를 제공해 사용 목적에 따라 다른 응답을 제공할 수 있고, 
트래픽에 대한 L4/L7 로드밸런서와 보안 인증서를 처리하는 기능을 제공합니다

Nginx 인그레스 컨트롤러 작동 순서

1. 사용자는 노드마다 설정된 노드포트를 통해 노드포트 서비스로 접속합니다. 이때 노드포트 서비스를 Nginx 인그레스 컨트롤러로 구성합니다

2. Nginx 인그레스 컨트롤러는 사용자의 접속 경로에 따라 적합한 클러스터 IP 서비스로 경로를 제공합니다

3. 클러스터 IP 서비스는 사용자를 해당 파드로 연결해 줍니다

> 인그레스 컨트롤러는 파드와 직접 통신이 불가 -> 노드포트 또는 로드밸런서 서비스와 연동되어야 한다

- expose 명령어를 통해 서비스 노출

외부와 통신하기 위해 클러스터 내부에서만 사용하는 파드를 클러스터 외부에 노출할 수 있는 구역으로 옮기는 것

### 3.3.3 : 클라우드에서 쉽게 구성 가능한 로드밸런서

앞에서 배운 연결 방식은 들어오는 요청을 모두 워커 노드의 노드포트를 통해 노드포트 서비스로 이동하고

이를 다시 쿠버네티스의 파드로 보내는 구조였습니다 (매우 비효율적)

이를 개선하기 위해 쿠버네티스는 **로드밸런서**라는 서비스 타입을 제공합니다

사용자 -> 클라우드 로드밸런서 -> 로드밸런서 서비스 -> 파드

### 3.3.4 : 온프레미스에서 로드밸런서를 제공하는 MetalLB

MetalLB는 특별한 네트워크 설정이나 구성이 있는 것이 아니라 기존의 L2 네트워크와 L3 네트워크로 로드밸런서를 구현합니다

이러한 이유들로 인해 네트워크를 새로 배워야 할 부담이 없으며 연동하기도 쉽습니다

MetalLB 컨트롤러는 작동 방식(프로토콜)을 정의하고 EXTERNAL-IP를 부여해 관리합니다

MetalLB 스피커는 정해진 작동 방식에 따라 경로를 만들 수 있도록 네트워크 정보를 광고하고 수집해 각 파드의 경로를 제공합니다

### 3.3.5 : 부하에 따라 자동으로 파드 수를 조절하는 HPA

지금까지는 사용자 1명이 파드에 접근하는 방법이었습니다.

사용자가 갑자기 늘어난다면 파드가 더 이상 감당할 수 없어서 서비스 불가라는 결과를 초래할 수 있습니다

쿠버네티스는 이를 방지하기 위해 부하량에 따라 디플로이먼트의 파드 수를 유동적으로 관리하는 기능을 제공합니다

이를 **HPA**라고 합니다

HPA는 자원을 요청할 때 메트릭 서버를 통해 계측값을 전달 받습니다

```json
HPA를 통해 늘어나느 파드 수 계산 방법

* resources의 CPU를 10m으로 설정, autoscale cpu-percent 를 50%로 가정

파드가 29m 부하를 받고 있다면

1개의 파드가 처리할 수 있는 부하는 10m이고, cpu 부하량이 50%가 넘으면 추가 파드를 생성해야 하므로

29m/5 = 5.8 = 6 이라는 숫자가 나와서 증가하는 파드의 수는 6입니다

> 이때 부하 총량을 가지고 HPA가 작동하기 때문에 일부 파드는 5m을 넘을 수도 있습니다
```

## 3.4 : 알아두면 쓸모 있는 쿠버네티스 오브젝트

- 데몬셋

디플로이먼트의 replicas가 노드 수 만큼 정해져 있는 형태
노드 하나당 파드 한 개만을 생성합니다

- 컨피그맵

이름 그대로 설정을 목적으로 사용하는 오브젝트입니다

인그레스에서는 설정을 위해 오브젝트를 인그레스로 선언했지만, MetalLB에서는 컨피그맵을 사용했습니다

인그레스는 오브젝트가 인그레스로 지정돼 있지만, MetalLB는 프로젝트 타입으로 정해진 오브젝트가 없어서 범용 설정으로 사용되는 컨피그맵을 지정했습니다

- PV와 PVC

때때로 파드에서 생성한 내용을 기록하고 보관하거나 모든 파드가 동일한 설정값을 유지하고 관리하기 위해 공유된 볼륨으로부터 공통된 설정을 가지고 올 수 있도록 설계해야 할 때도 있습니다

이를 위해 다양한 형태의 볼륨을 제공합니다

- 임시 : emptyDir
- 로컬 : host Path, local
- 원격 : persistentVolumneClaim, cinder, csi, flexVolumne, ...
- 특수 목적 : downwardAPI, configMap, secret, ...
- 클라우드 : awsElasticBlockStore, azureDisk, gcpPersistentDisk

쿠버네티스는 필요할 때 PVC를 요청해 사용합니다
PVC를 사용하려면 PV로 볼륨을 선언해야 합니다

간단하게 PV는 볼륨을 사용할 수 있게 준비하는 단계, PVC는 준비된 볼륨에서 일정 공간을 할당받는 것입니다

- 스테이트풀셋

파드가 replicas에 선언된 만큼 무작위로 생성될 뿐이었습니다
하지만, 파드가 만들어지는 이름과 순서를 예측해야 할 때가 있습니다

이때 스테이트풀셋을 사용합니다

volumeClaimTemplates 기능을 사용해 PVC를 자동으로 생성할 수 있고, 
각 파드가 순서대로 생성되기 때문에 고정된 이름, 볼륨 , 설정등을 가질 수 있습니다

다만, 효율성 면에서 좋은 구조가 아니므로 요구 사항에 맞게 적절히 사용하는 것이 좋습니다

```json
스테이트풀셋은 헤드리스 서비스로 노출?

일반적으로는 맞습니다. 헤드리스 서비스는 IP를 가지지 않는 서비스 타입으로 중요한 자원인 IP를 절약할 수 있을 뿐만 아니라, 
스테이트풀셋과 같은 상태를 가지고 있는 오브젝트를 모두 노출하지 않고 상태 값을 외부에 알리고 싶은 것만 선택적으로 노출하게 할 수 있습니다
```